@INPROCEEDINGS{czer11,
	author =	 {Czerwonka, J. and Das, R. and Nagappan, N. and
	Tarvo, A. and Teterev, A.},
	booktitle =	 {Software Testing, Verification and Validation
	(ICST), 2011 IEEE Fourth Intl. Conference
	on},
	title =	 {CRANE: Failure Prediction, Change Analysis and Test
	Prioritization in Practice -- Experiences from
	Windows},
	year =	 2011,
	month =	 {march},
	pages =	 {357 -366},
}

@book{Burden:1988,
 author = {Burden, Richard L. and Faires, J. Douglas},
 title = {Numerical Analysis: 4th Ed},
 year = {1989},
 isbn = {0-53491-585-X},
 publisher = {PWS Publishing Co.},
 address = {Boston, MA, USA},
} 



@INPROCEEDINGS{hihn15, 
author={J. Hihn and T. Menzies}, 
booktitle={2015 30th IEEE/ACM Intl. Conf. Automated Software Engineering Workshop (ASEW)}, 
title={Data Mining Methods and Cost Estimation Models: Why is it So Hard to Infuse New Ideas?}, 
year={2015}, 
pages={5-9}, 
keywords={data mining;software cost estimation;NASA;cost estimation models;data mining methods;effort estimation;software costing models;technology infusion;Cognitive science;Data mining;Estimation;NASA;Software;Solid modeling;Stakeholders;cost estimation;data mining;effort estimation;software;technology infusion}, 
doi={10.1109/ASEW.2015.27}, 
month={Nov},}

@inproceedings{ostrand04,
	author =	 {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell,
	Robert M.},
	title =	 {Where the bugs are},
	booktitle =	 {ISSTA '04: Proc. the 2004 ACM SIGSOFT
	Intl. symposium on Software testing and
	analysis},
	year =	 2004,
	pages =	 {86--96},
	publisher =	 {ACM},
	address =	 {New York, NY, USA},
}

@ARTICLE{Bel,
    author = "Richard Bellman",
     title = "A Markovian Decision Process",
   journal = "Indiana Univ. Math. J.",
  fjournal = "Indiana University Mathematics Journal",
    volume = 6,
      year = 1957,
     issue = 4,
     pages = "679--684",
      issn = "0022-2518",
     coden = "IUMJAB",
   mrclass = "",
}

@article{kaelbling98,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}

@book{norvig,
  title={Artificial Intelligence: A Modern Approach},
  author={Russell, Stuart and Norvig, Peter},
  publisher={Prentice-Hall, Egnlewood Cliffs},
  isbn={0-13-604259-7},
  year={1995}
}

@article{guo2009,
  title={Continuous-time Markov decision processes},
  author={Guo, Xianping and Hern{\'a}ndez-Lerma, On{\'e}simo},
  journal={Continuous-Time Markov Decision Processes},
  pages={9--18},
  year={2009},
  publisher={Springer}
}

@book{altman99,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@article{wooldridge95,
  title={Intelligent agents: Theory and practice},
  author={Wooldridge, Michael and Jennings, Nicholas R},
  journal={The knowledge engineering review},
  volume={10},
  number={2},
  pages={115--152},
  year={1995},
  publisher={Cambridge University Press}
}

@article{deb00a,
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T},
journal = {IEEE Transactions on Evolutionary Computation},
pages = {182--197},
title = {{A Fast Elitist Multi-Objective Genetic Algorithm: NSGA-II}},
volume = {6},
year = {2002}
}

@inproceedings{baier09,
  title={HTN planning with preferences},
  author={Baier, Shirin Sohrabi Jorge A and McIlraith, Sheila A},
  booktitle={21st Int. Joint Conf. on Artificial Intelligence},
  pages={1790--1797},
  year={2009}
}

@article{son06,
  title={Planning with preferences using logic programming},
  author={Son, Tran Cao and Pontelli, Enrico},
  journal={Theory and Practice of Logic Programming},
  volume={6},
  number={5},
  pages={559--607},
  year={2006},
  publisher={Cambridge University Press}
}

@article{strips,
  title={STRIPS: A new approach to the application of theorem proving to problem solving},
  author={Fikes, Richard E and Nilsson, Nils J},
  journal={Artificial intelligence},
  volume={2},
  number={3-4},
  pages={189--208},
  year={1971},
  publisher={Elsevier}
}

@article{Harman2009,
  title={Search based software engineering: A comprehensive analysis and review of trends techniques and applications},
  author={Harman, Mark and Mansouri, S Afshin and Zhang, Yuanyuan},
  journal={Department of Computer Science, King’s College London, Tech. Rep. TR-09-03},
  year={2009}
}

@article{Harman2011,
abstract = {The aim of Search Based Software Engineering (SBSE) research is to move software engineering problems from human-based search to machine-based search, using a variety of techniques from the metaheuristic search, operations research and evolutionary computation paradigms. The idea is to exploit humans creativity and machines tenacity and reliability, rather than requiring humans to perform the more tedious, error prone and thereby costly aspects of the engineering process. SBSE can also provide insights and decision support. This tutorial will present the reader with a step-by-step guide to the application of SBSE techniques to Software Engineering. It assumes neither previous knowledge nor experience with Search Based Optimisation. The intention is that the tutorial will cover sufficient material to allow the reader to become productive in successfully applying search based optimisation to a chosen Software Engineering problem of interest.},
author = {Harman, M and McMinn, P and {De Souza}, JT and Yoo, S},
doi = {10.1007/978-3-642-25231-0\_1},
journal = {Search},
pages = {1--59},
title = {{Search based software engineering: Techniques, taxonomy, tutorial}},
volume = {2012},
year = {2011}
}



@book{ghallab04,
  title={Automated Planning: theory and practice},
  author={Ghallab, Malik and Nau, Dana and Traverso, Paolo},
  year={2004},
  publisher={Elsevier}
}

@article{Menzies2007a,
	abstract = {Zhang and Zhang argue that predictors are useless unless they have high precison{\&}amp;recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.},
	author = {Menzies, Tim and Dekhtyar, Alex and Distefano, Justin and Greenwald, Jeremy},
	doi = {10.1109/TSE.2007.70721},
	file = {:C$\backslash$:/Users/Rahul Krishna/Documents/Mendeley Desktop/Menzies et al. - 2007 - Problems with precision A response to Comments on 'data mining static code attributes to learn defect predictors.pdf:pdf},
	issn = {0098-5589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Accuracy measures,Defect prediction,Empirical,Static code attributes},
	month = {sep},
	number = {9},
	pages = {637--640},
	title = {{Problems with Precision: A Response to "Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'"}},
	volume = {33},
	year = {2007}
}

@misc{Hassan17,
author="A. Hassan",
title="Remarks made during a presentation to the UCL Crest Open Workshop",
month="March",
year=2017
}

@inproceedings{turhan11,
	title={Empirical evaluation of mixed-project defect prediction models},
	author={Turhan, Burak and Tosun, Ay{\c{s}}e and Bener, Ay{\c{s}}e},
	booktitle={Software Engineering and Advanced Applications (SEAA), 2011 37th EUROMICRO Conf.},
	pages={396--403},
	year={2011},
	organization={IEEE}
}

@ARTICLE{koc11b,
	author =	 "E. Kocaguneli and T. Menzies and A. Bener and
	J. Keung",
	journal =	 "IEEE Transactions on Software Engineering",
	title =	 "Exploiting the Essential Assumptions of
	Analogy-Based Effort Estimation",
	year =	 2012,
	volume =	 28,
	issue =	 2,
	pages =	 "425-438",
	class =	 "hJ"
}

@inproceedings{linares2015optimizing,
title={Optimizing energy consumption of guis in android apps: A multi-objective approach},
author={Linares-V{\'a}squez, Mario and Bavota, Gabriele and C{\'a}rdenas, Carlos Eduardo Bernal and Oliveto, Rocco and Di Penta, Massimiliano and Poshyvanyk, Denys},
booktitle={Proc. 2015 10th Joint Meeting on Foundations of Software Engineering},
pages={143--154},
year={2015},
organization={ACM}
}

@inproceedings{linares2014mining,
	title={Mining energy-greedy API usage patterns in Android apps: an 
	empirical study},
	author={Linares-V{\'a}squez, Mario and Bavota, Gabriele and 
	Bernal-C{\'a}rdenas, Carlos and Oliveto, Rocco and Di Penta, Massimiliano 
	and Poshyvanyk, Denys},
	booktitle={Proc. 11th Working Conf. Mining Software 
	Repositories},
	pages={2--11},
	year={2014},
	organization={ACM}
}

@Inproceedings{export:208800,
	abstract     = {<p>In this paper, we present the results from two surveys 
	related to data science
	applied to software engineering. The first survey solicited questions that
	software engineers would like data scientists to investigate about software,
	about software processes and practices, and about software engineers. Our
	analyses resulted in a list of 145 questions grouped into 12 categories. The
	second survey asked a different pool of software engineers to rate these 145
	questions and identify the most important ones to work on first. Respondents
	favored questions that focus on how customers typically use their 
	applications.
	We also saw opposition to questions that assess the performance of 
	individual
	employees or compare them with one another. Our categorization and catalog 
	of 145
	questions can help researchers, practitioners, and educators to more easily 
	focus
	their efforts on topics that are important to the software industry.</p>
	
	<p>The data appendix for this paper is here:
	http://research.microsoft.com/apps/pubs/?id=200784.</p>},
	author       = {Andrew Begel and Thomas Zimmermann},
	booktitle    = {Proc. 36th Intl. Conf. 
	Software Engineering (ICSE
	2014)},
	month        = {June},
	publisher    = {ACM},
	title        = {Analyze This! 145 Questions for Data Scientists in Software 
	Engineering},
	year         = {2014},
}

@inproceedings{theisen15,
	year=2015,
	title="Approximating Attack Surfaces with Stack Traces",
	author="Christopher Theisen and  Kim Herzig and  Patrick Morrison and  
	Brendan Murphy and  Laurie Williams",
	booktitle="ICSE'15"
}

@article{12instability,
	abstract = {The goal of science is conclusion stability, i.e. to discover some effect X that holds in multiple situations. Sadly, there are all too few examples of stable conclusions in software engineering (SE). In fact, the typical result is conclusion instability where what is true for project one, does not hold for project two. We can find numerous studies of the following form: there is as much evidence for as against the argument that some aspect X adds value to a software project. Below are four examples of this type of problem which we believe to be endemic within SE.},
	author = {Menzies, Tim and Shepperd, Martin},
	doi = {10.1007/s10664-011-9193-5},
	file = {:Users/rkrsn/Documents/Mendeley Desktop/Menzies, Shepperd - 2012 - Special issue on repeatable results in software engineering prediction(2).pdf:pdf},
	issn = {1382-3256},
	journal = {Empir. Softw. Eng.},
	month = {feb},
	number = {1-2},
	pages = {1--17},
	title = {{Special issue on repeatable results in software engineering prediction}},
	volume = {17},
	year = {2012}
}

@article{Jorgensen04,
	title={A review of studies on expert estimation of software development effort},
	author={J{\o}rgensen, Magne},
	journal={Journal of Systems and Software},
	volume={70},
	number={1},
	pages={37--60},
	year={2004},
	publisher={Elsevier}
}

@article{kitchenham07,
	title={Cross versus within-company cost estimation studies: A systematic review},
	author={Kitchenham, Barbara A and Mendes, Emilia and Travassos, Guilherme H},
	journal={IEEE Transactions on Software Engineering},
	volume={33},
	number={5},
	year={2007},
	publisher={IEEE}
}

@inproceedings{Shepperd05,
	title={The consistency of empirical comparisons of regression and analogy-based software project cost prediction},
	author={Mair, Carolyn and Shepperd, Martin},
	booktitle={Empirical Software Engineering, 2005. 2005 Intl. Symposium on},
	pages={10--pp},
	year={2005},
	organization={IEEE}
}

@article{Zimmermann09,
	abstract = {Prediction of software defects works well within projects as long as there is a sufficient amount of data available to train any models. However, this is rarely the case for new software projects and for many companies. So far, only a few have studies focused on transferring prediction models from one project to another. In this paper, we study cross-project defect prediction models on a large scale. For 12 real-world applications, we ran 622 cross-project predictions. Our results indicate that cross-project prediction is a serious challenge, i.e., simply using models from projects in the same domain or with the same process does not lead to accurate predictions. To help software engineers choose models wisely, we identified factors that do influence the success of cross-project predictions. We also derived decision trees that can provide early estimates for precision, recall, and accuracy before a prediction is attempted.},
	author = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
	doi = {10.1145/1595696.1595713},
	isbn = {9781605580012},
	journal = {Proc. 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering on European software engineering conference and foundations of software engineering symposium - E},
	keywords = {Defect prediction,churn,cross-project,decision trees,defect prediction,logistic regression,prediction quality},
	pages = {91},
	title = {{Cross-project defect prediction}},
	year = {2009}
}

@article{krishna17a,
	abstract = {Context: Developers use bad code smells to guide code reorganization. Yet developers, text books, tools, and researchers disagree on which bad smells are important. Objective: To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code. Method: We introduce XTREE, a tool that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes. Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems). Evaluation: We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts. Results: Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations from previous studies. Further, XTREE endorses changes to very few code metrics, and the bad smell recommendations (learned from previous studies) are not universal to all software projects. Conclusion: Before undertaking a code reorganization based on a bad smell report, use a tool like XTREE to check and ignore any such operations that are useless; i.e. ones which lack evidence in the historical record that it is useful to make that change. Note that this use case applies to both manual code reorganizations proposed by developers as well as those conducted by automatic methods. This recommendation assumes that there is an historical record. If none exists, then the results of this paper could be used as a guide.},
	archivePrefix = {arXiv},
	arxivId = {1609.03614},
	author = {Krishna, Rahul and Menzies, Tim and Layman, Lucas},
	doi = {10.1016/j.infsof.2017.03.012},
	eprint = {1609.03614},
	issn = {09505849},
	journal = {Information and Software Technology},
	keywords = {bad smells,decision trees,performance prediction},
	month = {mar},
	title = {{Less is more: Minimizing code reorganization using XTREE}},
	year = {2017}
}

@inproceedings{krishna16,
	abstract = {Transfer learning: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple bellwether transfer learner. Given N data sets, we find which one produce the best predictions on all the others. This bellwether data set is then used for all subsequent predictions (or, until such time as its predictions start failing-- at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) bellwethers are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.},
	address = {New York, New York, USA},
	author = {Krishna, Rahul and Menzies, Tim and Fu, Wei},
	booktitle = {Proc. 31st IEEE/ACM Intl. Conf. Automated Software Engineering - ASE 2016},
	doi = {10.1145/2970276.2970339},
	file = {:home/rkrsn/Documents/Mendeley Desktop/Krishna, Menzies, Fu - 2016 - Too much automation the bellwether effect and its implications for transfer learning.pdf:pdf},
	isbn = {9781450338455},
	keywords = {data mining,defect prediction,transfer learning},
	mendeley-groups = {Bellwether Journal},
	pages = {122--131},
	publisher = {ACM Press},
	title = {{Too much automation? the bellwether effect and its implications for transfer learning}},
	year = {2016}
}

@misc{menzies2016promise,
  title={The promise repository of empirical software engineering data. North Carolina State University, Department of Computer Science},
  author={Menzies, Tim and Krishna, Rahul and Pryor, David},
  year={2016}
}

@ARTICLE{krishna17b,
abstract="Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the
conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use
of “bellwethers”: given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate
conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover
(just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just
the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort
estimation, and bad smell detection. We hence recommend using bellwethers as a
baseline method
for transfer learning against which future work should be compared.",
author={Krishna, Rahul and Menzies, Tim},
journal={IEEE Transactions on Software Engineering},
title={Bellwethers: A Baseline Method For Transfer Learning},
year={2018},
volume={},
number={},
pages={1-1},
keywords={Analytical models;Benchmark testing;Complexity theory;Estimation;Software;Software engineering;Task analysis;Bad smells;Defect Prediction;Effort Estimation;Issue Close Time;Prediction;Transfer learning},
doi={10.1109/TSE.2018.2821670},
ISSN={0098-5589},
month={},}



@ARTICLE{shatnawi, 
	author={R. Shatnawi}, 
	journal={IEEE Transactions on Software Engineering}, 
	title={A Quantitative Investigation of the Acceptable Risk Levels of Object-Oriented Metrics in Open-Source Systems}, 
	year={2010}, 
	volume={36}, 
	number={2}, 
	pages={216-225}, 
	keywords={decision trees;object-oriented programming;public domain software;software fault tolerance;software maintenance;software metrics;statistical analysis;Chidamber and Kemerer metrics;Eclipse project version 2.1;data distribution parameters;decision trees;logistic regression;object-oriented metrics;open source systems;software complexity;software design;software metrics;statistical model;threshold values;CK metrics;Object-oriented programming;open-source software.;product metrics;threshold values}, 
	doi={10.1109/TSE.2010.9}, 
	ISSN={0098-5589}, 
	month={March},}

@inproceedings{alves,
	author = {Alves, Tiago L. and Ypma, Christiaan and Visser, Joost},
	booktitle = {2010 IEEE Int. Conf. Softw. Maint.},
	doi = {10.1109/ICSM.2010.5609747},
	isbn = {978-1-4244-8630-4},
	issn = {10636773},
	mendeley-groups = {OO Metric Thresholds},
	month = {sep},
	pages = {1--10},
	publisher = {IEEE},
	title = {{Deriving metric thresholds from benchmark data}},
	year = {2010}
}

@inproceedings{oliveira,
	abstract = {Establishing credible thresholds is a central challenge for promoting source code metrics as an effective instrument to control the internal quality of software systems. To address this challenge, we propose the concept of relative thresholds for evaluating metrics data following heavy-tailed distributions. The proposed thresholds are relative because they assume that metric thresholds should be followed by most source code entities, but that it is also natural to have a number of entities in the “long-tail” that do not follow the defined limits. In the paper, we describe an empirical method for extracting relative thresholds from real systems. We also report a study on applying this method in a corpus with 106 systems. Based on the results of this study, we argue that the proposed thresholds express a balance between real and idealized design practices.},
	author = {Oliveira, Paloma and Valente, Marco Tulio and Lima, Fernando Paim},
	booktitle = {2014 Software Evolution Week - IEEE Conf. Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)},
	doi = {10.1109/CSMR-WCRE.2014.6747177},
	file = {:home/rkrsn/Documents/Mendeley Desktop/Oliveira, Valente, Lima - 2014 - Extracting relative thresholds for source code metrics.pdf:pdf},
	isbn = {978-1-4799-3752-3},
	issn = {1063-6773},
	keywords = {Relative thresholds,Software measurement,Software quality,Source code metrics},
	mendeley-groups = {ASE 2017},
	month = {feb},
	pages = {254--263},
	publisher = {IEEE},
	title = {{Extracting relative thresholds for source code metrics}},
	year = {2014}
}

@inproceedings{fontana,
	abstract = {Code smells are archetypes of design shortcomings in the code that can potentially cause problems during maintenance. One known approach for detecting code smells is via detection rules: a combination of different object-oriented metrics with pre-defined threshold values. The usage of inadequate thresholds when using this approach could lead to either having too few observations (too many false negatives) or too many observations (too many false positives). Furthermore, without a clear methodology for deriving thresholds, one is left with those suggested in literature (or by the tool vendors), which may not necessarily be suitable to the context of analysis. In this paper, we propose a data-driven (i.e., Benchmark-based) method to derive threshold values for code metrics, which can be used for implementing detection rules for code smells. Our method is transparent, repeatable and enables the extraction of thresholds that respect the statistical properties of the metric in question (such as scale and distribution). Thus, our approach enables the calibration of code smell detection rules by selecting relevant systems as benchmark data. To illustrate our approach, we generated a benchmark dataset based on 74 systems of the Qualitas Corpus, and extracted the thresholds for five smell detection rules.},
	author = {{Arcelli Fontana}, Francesca and Ferme, Vincenzo and Zanoni, Marco and Yamashita, Aiko},
	booktitle = {2015 IEEE/ACM 6th Intl. Workshop on Emerging Trends in Software Metrics},
	doi = {10.1109/WETSoM.2015.14},
	file = {:home/rkrsn/Documents/Mendeley Desktop/Arcelli Fontana et al. - 2015 - Automatic Metric Thresholds Derivation for Code Smell Detection.pdf:pdf},
	isbn = {978-1-4673-7103-2},
	mendeley-groups = {ASE 2017},
	month = {may},
	pages = {44--53},
	publisher = {IEEE},
	title = {{Automatic Metric Thresholds Derivation for Code Smell Detection}},
	volume = {2015-August},
	year = {2015}
}

@article{Hall2011,
	abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs and improve the quality of software. Objective: We investigate how the context of models, the independent variables used and the modelling techniques applied, influence the performance of fault prediction models. Method:We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesise the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modelling techniques such as Na{\"{\i}}ve Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology and performance comprehensively.},
	author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
	doi = {10.1109/TSE.2011.103},
	isbn = {9781612081656},
	issn = {0098-5589},
	journal = {IEEE Transactions on Software Engineering},
	number = {6},
	pages = {1276--1304},
	title = {{A Systematic Review of Fault Prediction Performance in Software Engineering}},
	volume = {38},
	year = {2011}
}

@article{lessmann08,
	abstract = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
	author = {Lessmann, Stefan and Baesens, Bart and Mues, C. and Pietsch, S.},
	doi = {10.1109/TSE.2008.35},
	isbn = {00985589 (ISSN)},
	issn = {0098-5589},
	journal = {IEEE Trans. Softw. Eng.},
	keywords = {Complexity measures,Data mining,Formal methods,Statistical methods,benchmark testing,benchmarking classification models,code attributes,fault-prone modules,metric-based classification,predictive classification models,proprietary data sets,software defect prediction,software quality,statistical testing,statistical testing procedures,testing efficiency},
	month = {jul},
	number = {4},
	pages = {485--496},
	title = {{Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings}},
	volume = {34},
	year = {2008}
}

@inproceedings{zimm,
	title={Cross-project defect prediction: a large scale experiment on data vs. domain vs. process},
	author={Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
	booktitle={Proc. the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
	pages={91--100},
	year={2009},
	organization={ACM}
}

@book{halstead77,
	title={Elements of software science},
	author={Halstead, Maurice Howard},
	volume={7},
	year={1977},
	publisher={Elsevier New York}
}

@article{mccabe76,
	title={A complexity measure},
	author={McCabe, Thomas J},
	journal={IEEE Transactions on software Engineering},
	pages={308--320},
	year={1976},
	publisher={IEEE}
}

@inproceedings{chapman02,
	title={The relationship of cyclomatic complexity, essential complexity and error rates},
	author={Chapman, M and Solomon, D},
	booktitle={Proc. NASA Software Assurance Symposium, Coolfont Resort and Conference Center in Berkley Springs, West Virginia},
	year={2002}
}

@inproceedings{nagappan05,
	title={Static analysis tools as early indicators of pre-release defect density},
	author={Nagappan, Nachiappan and Ball, Thomas},
	booktitle={Proc. 27th Intl. Conf. Software engineering},
	pages={580--586},
	year={2005},
	organization={ACM}
}

@article{hall2000,
	title={Software evolution: code delta and code churn},
	author={Hall, Gregory A and Munson, John C},
	journal={Journal of Systems and Software},
	volume={54},
	number={2},
	pages={111--118},
	year={2000},
	publisher={Elsevier}
}

@article{fenton00,
	title={Quantitative analysis of faults and failures in a complex software system},
	author={Fenton, Norman E. and Ohlsson, Niclas},
	journal={IEEE Transactions on Software engineering},
	volume={26},
	number={8},
	pages={797--814},
	year={2000},
	publisher={IEEE}
}

@article{shepperd94,
	title={A critique of three metrics},
	author={Shepperd, Martin and Ince, Darrel C},
	journal={Journal of Systems and Software},
	volume={26},
	number={3},
	pages={197--210},
	year={1994},
	publisher={Elsevier}
}

@article{localvsglobal,
	abstract = {Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data. [ABSTRACT FROM PUBLISHER]},
	author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
	doi = {10.1109/TSE.2012.83},
	file = {:home/rkrsn/Documents/Mendeley Desktop/Menzies et al. - 2013 - Local versus Global Lessons for Defect Prediction and Effort Estimation.pdf:pdf},
	isbn = {9781605580791},
	issn = {0098-5589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {Data mining,clustering,defect prediction,effort estimation},
	month = {jun},
	number = {6},
	pages = {822--834},
	title = {{Local versus Global Lessons for Defect Prediction and Effort Estimation}},
	volume = {39},
	year = {2013}
}

@article{turhan_drift,
	title={On the dataset shift problem in software engineering prediction models},
	author={Turhan, Burak},
	journal={Empirical Software Engineering},
	volume={17},
	number={1-2},
	pages={62--74},
	year={2012},
	publisher={Springer}
}

@article{azzeh,
	title={A replicated assessment and comparison of adaptation techniques for analogy-based effort estimation},
	author={Azzeh, Mohammad},
	journal={Empirical Software Engineering},
	volume={17},
	number={1-2},
	pages={90--127},
	year={2012},
	publisher={Springer}
}

@article{robles,
	title={On the reproducibility of empirical software engineering studies based on data retrieved from development repositories},
	author={Gonz{\'a}lez-Barahona, Jes{\'u}s M and Robles, Gregorio},
	journal={Empirical Software Engineering},
	volume={17},
	number={1-2},
	pages={75--89},
	year={2012},
	publisher={Springer}
}

@article{fi,
	title={Multi-interval discretization of continuous-valued attributes for classification learning},
	author={Fayyad, Usama and Irani, Keki},
	journal={NASA JPL Archives},
	year={1993}
}

@mastersthesis{papa13,
  author="Vasil Papakroni",
  title="Data Carving: Identifying and Removing Irrelevancies in the Data",
  school="Lane Department of Computer Science and Electrical Engineering, West Virginia Unviersity",
  year=2013
}

@inproceedings{dougherty,
	title={Supervised and unsupervised discretization of continuous features},
	author={Dougherty, James and Kohavi, Ron and Sahami, Mehran and others},
	booktitle={Machine learning: Proc. 12th Intl. conference},
	volume={12},
	pages={194--202},
	year={1995}
}

@inproceedings{koca10,
	title={When to use data from other projects for effort estimation},
	author={Kocaguneli, Ekrem and Gay, Gregory and Menzies, Tim and Yang, Ye and Keung, Jacky W},
	booktitle={Proc. IEEE/ACM Intl. Conf. Automated software engineering},
	pages={321--324},
	year={2010},
	organization={ACM}
}

@inproceedings{chen05,
	title={Feature subset selection can improve software cost estimation accuracy},
	author={Chen, Zhihao and Menzies, Tim and Port, Dan and Boehm, Barry},
	booktitle={ACM SIGSOFT Software Engineering Notes},
	volume={30},
	pages={1--6},
	year={2005},
	organization={ACM}
}

@article{smote,
	title={SMOTE: synthetic minority over-sampling technique},
	author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
	journal={Journal of artificial intelligence research},
	volume={16},
	pages={321--357},
	year={2002}
}

@book{rubin,
	title={Statistical analysis with missing data},
	author={Little, Roderick JA and Rubin, Donald B},
	year={2014},
	publisher={John Wiley \& Sons}
}

@book{witten,
  title={Data Mining: Practical machine learning tools and techniques},
  author={Witten, Ian H and Frank, Eibe and Hall, Mark A and Pal, Christopher J},
  year={2016},
  publisher={Morgan Kaufmann}
}

@article{hall03,
  title={Benchmarking attribute selection techniques for discrete class data mining},
  author={Hall, Mark A and Holmes, Geoffrey},
  journal={IEEE Transactions on Knowledge and Data engineering},
  volume={15},
  number={6},
  pages={1437--1447},
  year={2003},
  publisher={IEEE}
}

@article{kitchenham01,
  title={What accuracy statistics really measure},
  author={Kitchenham, Barbara A and Pickard, Lesley M and MacDonell, Stephen G. and Shepperd, Martin J.},
  journal={IEE Proceedings-Software},
  volume={148},
  number={3},
  pages={81--85},
  year={2001},
  publisher={IET}
}

@article{myrtveit05,
  title={Reliability and validity in comparative studies of software prediction models},
  author={Myrtveit, Ingunn and Stensrud, Erik and Shepperd, Martin},
  journal={IEEE Transactions on Software Engineering},
  volume={31},
  number={5},
  pages={380--391},
  year={2005},
  publisher={IEEE}
}

@article{shepperd97,
  title={Estimating software project effort using analogies},
  author={Shepperd, Martin and Schofield, Chris},
  journal={IEEE Transactions on software engineering},
  volume={23},
  number={11},
  pages={736--743},
  year={1997},
  publisher={IEEE}
}


@article{DAmbros2012,
abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
doi = {10.1007/s10664-011-9173-9},
isbn = {1066401191739},
issn = {1382-3256},
journal = {Empir. Softw. Eng.},
keywords = {Change metrics,Defect prediction,Source code metrics},
month = {aug},
number = {4-5},
pages = {531--577},
title = {{Evaluating defect prediction approaches: a benchmark and an extensive comparison}},
volume = {17},
year = {2012}
}

@inproceedings{Wu2011,
abstract = {Software defect information, including links between bugs and committed changes, plays an important role in software maintenance such as measuring quality and predicting defects. Usually, the links are automatically mined from change logs and bug reports using heuristics such as searching for specific keywords and bug IDs in change logs. However, the accuracy of these heuristics depends on the quality of change logs. Bird et al. found that there are many missing links due to the absence of bug references in change logs. They also found that the missing links lead to biased defect information, and it affects defect prediction performance. We manually inspected the explicit links, which have explicit bug IDs in change logs and observed that the links exhibit certain features. Based on our observation, we developed an automatic link recovery algorithm, ReLink, which automatically learns criteria of features from explicit links to recover missing links. We applied ReLink to three open source projects. ReLink reliably identified links with 89{\%} precision and 78{\%} recall on average, while the traditional heuristics alone achieve 91{\%} precision and 64{\%} recall. We also evaluated the impact of recovered links on software maintainability measurement and defect prediction, and found the results of ReLink yields significantly better accuracy than those of traditional heuristics. {\textcopyright} 2011 ACM.},
address = {New York, New York, USA},
author = {Wu, Rongxin and Zhang, Hongyu and Kim, Sunghun and Cheung, Shing-Chi},
booktitle = {Proc. 19th ACM SIGSOFT Symp. 13th Eur. Conf. Found. Softw. Eng. - SIGSOFT/FSE '11},
doi = {10.1145/2025113.2025120},
isbn = {9781450304436},
issn = {9781450304436},
keywords = {Bugs,Changes,Computer software maintenance,Data quality,Defects,Forecasting,Maintainability,Mining software repository,Missing links,Program debugging,Recovery,Software engineering,and bug ids in,bird et al,bug,change logs,found that there are,however,logs,many missing links due,references in change logs,the accuracy of,the missing links,the quality of change,these heuristics depends on,they also found that,to the absence of},
pages = {15},
publisher = {ACM Press},
title = {{ReLink}},
year = {2011}
}

@article{basili1996validation,
  title={A validation of object-oriented design metrics as quality indicators},
  author={Basili, Victor R and Briand, Lionel C and Melo, Walc{\'e}lio L},
  journal={Software Engineering, IEEE Transactions on},
  volume={22},
  number={10},
  pages={751--761},
  year={1996},
  publisher={IEEE}
}

@article{ohlsson1996predicting,
  title={Predicting fault-prone software modules in telephone switches},
  author={Ohlsson, Niclas and Alberg, Hans},
  journal={Software Engineering, IEEE Transactions on},
  volume={22},
  number={12},
  pages={886--894},
  year={1996},
  publisher={IEEE}
}

@inproceedings{kim2011dealing,
  title={Dealing with noise in defect prediction},
  author={Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
  booktitle={Software Engineering (ICSE), 2011 33rd Intl. Conf.},
  pages={481--490},
  year={2011},
  organization={IEEE}
}


@inproceedings{Jureczko2010,
abstract = {Background: This paper describes an analysisthat was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary perfomed before showed the need for a further in-depth analysis study in order to identify project clusters. Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency. Method: Hierarchical and k-means clustering, as well obtained clusters were investigated with as Kohonen's neural network was used to find groups of similar projects. The the discriminant analysis. For each of the identified group a statistical analysis has been conducted in order to distinguish whether this group really exists. Two defect prediction models were created for each of the identified groups. The first one was based on the projects that belong to a given group, and the second one - on all the projects. Then, both models were applied to all versions of projects from the investigated group. If the predictions from the model based on projects that belong to the identified group are significantly better than the all-projects model (the mean values were compared and statistical tests were used), we conclude that the group really exists. Results: Six different clusters were identified and the existence of two of them was statistically proven: 1) cluster proprietary B – T=19, p=0.035, r=0.40; 2) cluster proprietary/open – t(17)=3.18, p=0.05, r=0.59. The obtained effect sizes (r) represent large effects according to Cohen's benchmark, which is a substantial finding. Conclusions: The two identified clusters were described and compared with results obtained by other researchers. The results of this work makes next step towards defining formal methods of reuse defect prediction models by identifying groups of projects within which the same defect prediction model may be used. Furthermore, a method of clustering was suggested and applied.},
address = {New York, New York, USA},
author = {Jureczko, Marian and Madeyski, Lech},
booktitle = {Proc. 6th Int. Conf. Predict. Model. Softw. Eng. - PROMISE '10},
doi = {10.1145/1868328.1868342},
isbn = {9781450304047},
keywords = {clustering,defect prediction,design metrics,size metrics},
pages = {1},
publisher = {ACM Press},
title = {{Towards identifying software project clusters with regard to defect prediction}},
year = {2010}
}

@article {bender99,
author = {Bender, Ralf},
title = {Quantitative Risk Assessment in Epidemiological Studies Investigating Threshold Effects},
journal = {Biometrical Journal},
volume = {41},
number = {3},
publisher = {WILEY-VCH Verlag Berlin GmbH},
issn = {1521-4036},
pages = {305--319},
keywords = {Benchmark values, Binary data, Epidemiological studies, Logistic regression, Quantitative risk assessment, Threshold model},
year = {1999},
}

@inproceedings{baxter,
  title={Understanding the shape of Java software},
  author={Baxter, Gareth and Frean, Marcus and Noble, James and Rickerby, Mark and Smith, Hayden and Visser, Matt and Melton, Hayden and Tempero, Ewan},
  booktitle={ACM Sigplan Notices},
  volume={41},
  pages={397--412},
  year={2006},
  organization={ACM}
}

@article{louridas,
  title={Power laws in software},
  author={Louridas, Panagiotis and Spinellis, Diomidis and Vlachos, Vasileios},
  journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume={18},
  number={1},
  pages={2},
  year={2008},
  publisher={ACM}
}

@inproceedings{Cheng10,
 author = {Cheng, Betty and Jensen, Adam   },
 title = {On the Use of Genetic Programming for Automated Refactoring and the Introduction of Design Patterns},
 booktitle = {Proc. 12th Annual Conf. Genetic and Evolutionary Computation},
 series = {GECCO '10},
 year = {2010},
 isbn = {978-1-4503-0072-8},
 location = {Portland, Oregon, USA},
 pages = {1341--1348},
 numpages = {8},
 doi = {10.1145/1830483.1830731},
 acmid = {1830731},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design patterns, evolutionary computation, intelligent search, object-oriented design, refactoring, search-based software engineering, software metrics},
} 
[download]

@article{OKeeffe08,
 author = {O'Keeffe, Mark and Cinn{\'e}ide, Mel \'{O}},
 title = {Search-based Refactoring: An Empirical Study},
 journal = {J. Softw. Maint. Evol.},
 issue_date = {September 2008},
 volume = {20},
 number = {5},
 month = sep,
 year = {2008},
 issn = {1532-060X},
 pages = {345--364},
 numpages = {20},
 doi = {10.1002/smr.v20:5},
 acmid = {1416585},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
 keywords = {automated design improvement, refactoring tools, search-based software engineering},
} 



@inproceedings{OKeeffe07,
 author = {O'Keeffe, Mark Kent and Cinneide, Mel O.},
 title = {Getting the Most from Search-based Refactoring},
 booktitle = {Proc. 9th Annual Conf. Genetic and Evolutionary Computation},
 series = {GECCO '07},
 year = {2007},
 isbn = {978-1-59593-697-4},
 location = {London, England},
 pages = {1114--1120},
 numpages = {7},
 doi = {10.1145/1276958.1277177},
 acmid = {1277177},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automated design improvement, object-oriented product metrics, refactoring, search-based software engineering},
} 

@Inbook{Moghadam2011,
author="Moghadam, Iman Hemati",
chapter="Multi-level Automated Refactoring Using Design Exploration",
title="Search Based Software Engineering: Third Intl. Symposium, SSBSE 2011, Szeged, Hungary, September 10-12, 2011. Proceedings",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="70--75",
isbn="978-3-642-23716-4",
doi="10.1007/978-3-642-23716-4_9"
}

@inproceedings{Mkaouer14,
 author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy and \'{O} Cinn{\'e}ide, Mel},
 title = {Recommendation System for Software Refactoring Using Innovization and Interactive Dynamic Optimization},
 booktitle = {Proc. 29th ACM/IEEE Intl. Conf. Automated Software Engineering},
 series = {ASE '14},
 year = {2014},
 isbn = {978-1-4503-3013-8},
 location = {Vasteras, Sweden},
 pages = {331--336},
 numpages = {6},
 doi = {10.1145/2642937.2642965},
 acmid = {2642965},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {refactoring, search based software engineering, software quality},
} 

@article{Bansiya02,
 author = {Bansiya, Jagdish and Davis, Carl G.},
 title = {A Hierarchical Model for Object-Oriented Design Quality Assessment},
 journal = {IEEE Trans. Softw. Eng.},
 issue_date = {January 2002},
 volume = {28},
 number = {1},
 month = jan,
 year = {2002},
 issn = {0098-5589},
 pages = {4--17},
 numpages = {14},
 doi = {10.1109/32.979986},
 acmid = {513066},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {quality model, quality attributes, design metrics, product metrics, object-oriented metrics},
} 

@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre- lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proc. Thirteenth Interna- tional conference, ∗∗∗, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression. Keywords:},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023\%2FA\%3A1010933404324},
author = {Breiman, L},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023\%2FA\%3A1010933404324},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
pmid = {21816105},
primaryClass = {http:},
title = {{Random forests}},
year = {2001}
}

@inproceedings{Nam13,
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. © 2013 IEEE.},
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
booktitle = {Proc. Intl. Conf. on Software Engineering},
doi = {10.1109/ICSE.2013.6606584},
isbn = {9781467330763},
issn = {02705257},
keywords = {cross-project defect prediction,empirical software engineering,transfer learning},
pages = {382--391},
title = {{Transfer defect learning}},
year = {2013}
}

@inproceedings{Nam15,
address = {New York, New York, USA},
author = {Nam, Jaechang and Kim, Sunghun},
booktitle = {Proc. 2015 10th Jt. Meet. Found. Softw. Eng. - ESEC/FSE 2015},
doi = {10.1145/2786805.2786814},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Nam, Kim - Heterogeneous defect prediction - 2015.pdf:pdf},
isbn = {9781450336758},
keywords = {defect prediction,heterogeneous metrics,quality assurance},
mendeley-groups = {ASE 2016},
pages = {508--519},
publisher = {ACM Press},
title = {{Heterogeneous defect prediction}},
year = {2015}
}

@article{fu,
 title="Tuning for Software Analytics: is it Really Necessary?",
author="W. Fu and T. Menzies and X. Shen",
journal="Information and Software Technology (submitted)",
year=2016
}

@article{Ma2012,
    abstract = {Context: Software defect prediction studies usually built
    models using within-company data, but very few focused on the prediction
    models trained with cross-company data. It is difficult to employ these
    models which are built on the within-company data in practice, because of
    the lack of these local data repositories. Recently, transfer learning has
    attracted more and more attention for building classifier in target domain
    using the data from related source domain. It is very useful in cases when
    distributions of training and test instances differ, but is it appropriate
    for cross-company software defect prediction? Objective: In this paper, we
    consider the cross-company defect prediction scenario where source and
    target data are drawn from different companies. In order to harness cross
    company data, we try to exploit the transfer learning method to build
    faster and highly effective prediction model. Method: Unlike the prior
    works selecting training data which are similar from the test data, we
    proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the
    information of all the proper features in training data. Our solution
    estimates the distribution of the test data, and transfers cross-company
    data information into the weights of the training data. On these weighted
    data, the defect prediction model is built. Results: This article presents
    a theoretical analysis for the comparative methods, and shows the
    experiment results on the data sets from different organizations. It
    indicates that TNB is more accurate in terms of AUC (The area under the
    receiver operating characteristic curve), within less runtime than the
    state of the art methods. Conclusion: It is concluded that when there are
    too few local training data to train good classifiers, the useful
    knowledge from different-distribution training data on feature level may
    help. We are optimistic that our transfer learning method can guide
    optimal resource allocation strategies, which may reduce software testing
    cost and increase effectiveness of software testing process.
    {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
    author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
    doi = {10.1016/j.infsof.2011.09.007},
    issn = {09505849},
    journal = {Information and Software Technology},
    keywords = {Different distribution,Machine learning,Naive Bayes,Software
    defect prediction,Transfer learning},
    number = {3},
    pages = {248--256},
    title = {{Transfer learning for cross-company software defect prediction}},
    volume = {54},
    year = {2012}
}

@ARTICLE{fu18, 
author={J. Nam and W. Fu and S. Kim and T. Menzies and L. Tan}, 
journal={IEEE Transactions on Software Engineering}, 
title={Heterogeneous Defect Prediction}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
keywords={Data models;NASA;Predictive models;Software;Software metrics;Training;defect prediction;heterogeneous metrics;quality assurance;transfer learning}, 
doi={10.1109/TSE.2017.2720603}, 
ISSN={0098-5589}, 
month={},}

@article{turhan09,
  title={On the relative value of cross-company and within-company data for defect prediction},
  author={Turhan, Burak and Menzies, Tim and Bener, Ay{\c{s}}e B and Di Stefano, Justin},
  journal={Empirical Software Engineering},
  volume={14},
  number={5},
  pages={540--578},
  year={2009},
  publisher={Springer}
}

@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
eprint = {1201.0490},
file = {::},
isbn = {1532-4435},
issn = {15324435},
journal = { Journal Machine Learning },
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2012}
}

@article{mittas13,
  author =	{Nikolaos Mittas and Lefteris Angelis},
  title =	{Ranking and Clustering Software Cost Estimation
                  Models through a Multiple Comparisons Algorithm},
  journal =	{IEEE Trans. Software Eng.},
  volume =	39,
  number =	4,
  year =	2013,
  pages =	{537-551},
}

@book{efron93,
  author =	"Efron, Bradley and Tibshirani, Robert J",
  title =	"An introduction to the bootstrap",
  publisher =	"Chapman and Hall",
  address =	"London",
  series =	"Mono. Stat. Appl. Probab.",
  year =	1993,
}

@INPROCEEDINGS{arcuri11,
author={Arcuri, A. and Briand, L.}, 
booktitle={ICSE'11},
title={A practical guide for using statistical tests to assess randomized algorithms in software engineering}, 
year={2011}, 
pages={1-10}}

@article{shepperd12a,
  author =	 {Martin J. Shepperd and Steven G. MacDonell},
  title =	 {Evaluating prediction systems in software project
                  estimation},
  journal =	 {Information {\&} Software Technology},
  volume =	 54,
  number =	 8,
  year =	 2012,
  pages =	 {820-827},
}

@article{kampenes07,
  author =	 {Vigdis By Kampenes and Tore Dyb{\aa} and Jo Erskine
                  Hannay and Dag I. K. Sj{\o}berg},
  title =	 {A systematic review of effect size in software
                  engineering experiments},
  journal =	 {Information {\&} Software Technology},
  volume =	 49,
  number =	 {11-12},
  year =	 2007,
  pages =	 {1073-1086},
}

@inproceedings{Kocaguneli2013:ep,
author = {Kocaguneli, Ekrem and Zimmermann, Thomas and Bird, Christian and Nagappan, Nachiappan and Menzies, Tim},
booktitle = {Proc. Intl. Conf. on Software Engineering},
doi = {10.1109/ICSE.2013.6606637},
isbn = {9781467330763},
issn = {02705257},
pages = {882--890},
title = {{Distributed development considered harmful?}},
year = {2013}
}

@INPROCEEDINGS{ma07, 
author={Y. Ma and B. Cukic}, 
booktitle={Predictor Models in Software Engineering, 2007. PROMISE'07: ICSE Workshops 2007. Intl. Workshop on}, 
title={Adequate and Precise Evaluation of Quality Models in Software Engineering Studies}, 
year={2007}, 
pages={1-1}, 
keywords={software engineering;statistical analysis;fault-proneness prediction;performance assessment;predictive models;quality models evaluation;software engineering studies;statistical techniques;Classification tree analysis;Computer science;Costs;Genetic algorithms;Logistics;Neural networks;Predictive models;Software engineering;Software metrics;Software quality}, 
doi={10.1109/PROMISE.2007.1}, 
month={May},}

@article{fu16,
 title="Tuning for Software Analytics: is it Really Necessary?",
author="W. Fu and T. Menzies and X. Shen",
journal="Information and Software Technology (submitted)",
year=2016,
note="Read on-line at https://goo.gl/Jp5VIm."
}

@article{swets1988measuring,
  title={Measuring the accuracy of diagnostic systems},
  author={Swets, John A},
  journal={Science},
  volume={240},
  number={4857},
  pages={1285},
  year={1988},
  publisher={The American Association for the Advancement of Science}
}

@book{duda2012pattern,
  title={Pattern classification},
  author={Duda, Richard O and Hart, Peter E and Stork, David G},
  year={2012},
  publisher={John Wiley \& Sons}
}

@book{menzies2014sharing,
  title={Sharing data and models in software engineering},
  author={Menzies, Tim and Kocaguneli, Ekrem and Turhan, Burak and Minku, Leandro and Peters, Fayola},
  year={2014},
  publisher={Morgan Kaufmann}
}

@inproceedings{ghotra2015revisiting,
  title={Revisiting the impact of classification techniques on the performance of defect prediction models},
  author={Ghotra, Baljinder and McIntosh, Shane and Hassan, Ahmed E},
  booktitle={37th ICSE-Volume 1},
  pages={789--800},
  year={2015},
  organization={IEEE Press}
}

@Article{herbold2016,
author="Herbold, Steffen
and Trautsch, Alexander
and Grabowski, Jens",
title="Global vs. local models for cross-project defect prediction",
journal="Empirical Software Engineering",
year="2016",
pages="1--37",
abstract="Although researchers invested significant effort, the performance of defect prediction in a cross-project setting, i.e., with data that does not come from the same project, is still unsatisfactory. A recent proposal for the improvement of defect prediction is using local models. With local models, the available data is first clustered into homogeneous regions and afterwards separate classifiers are trained for each homogeneous region. Since the main problem of cross-project defect prediction is data heterogeneity, the idea of local models is promising. Therefore, we perform a conceptual replication of the previous studies on local models with a focus on cross-project defect prediction. In a large case study, we evaluate the performance of local models and investigate their advantages and drawbacks for cross-project predictions. To this aim, we also compare the performance with a global model and a transfer learning technique designed for cross-project defect predictions. Our findings show that local models make only a minor difference in comparison to global models and transfer learning for cross-project defect prediction. While these results are negative, they provide valuable knowledge about the limitations of local models and increase the validity of previously gained research results.",
issn="1573-7616",
doi="10.1007/s10664-016-9468-y"
}

@inproceedings{bettenburg12,
  title={Think locally, act globally: Improving defect and effort prediction models},
  author={Bettenburg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E},
  booktitle={Mining Software Repositories (MSR)},
  pages={60--69},
  year={2012},
  organization={IEEE}
}

@article{bettenburg14,
  title={Towards improving statistical modeling of software engineering data: think locally, act globally!},
  author={Bettenburg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E},
  journal={Empirical Software Engineering},
  volume={20},
  number={2},
  pages={294--335},
  year={2015},
  publisher={Springer}
}

@inproceedings{scanniello,
  title={Class level fault prediction using software clustering},
  author={Scanniello, Giuseppe and Gravino, Carmine and Marcus, Andrian and Menzies, Tim},
  booktitle={Automated Software Engineering (ASE), 2013},
  pages={640--645},
  year={2013},
  organization={IEEE}
}

@inproceedings{sayyad13,
  title={Scalable product line configuration: A straw to break the camel's back},
  author={Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
  booktitle={Automated Software Engineering (ASE), 2013 IEEE/ACM 28th Intl. Conf.},
  pages={465--474},
  year={2013},
  organization={IEEE}
}

@INPROCEEDINGS{henard15, 
author={C. Henard and M. Papadakis and M. Harman and Y. Le Traon}, 
booktitle={2015 IEEE/ACM 37th IEEE Intl. Conf. Software Engineering}, 
title={Combining Multi-Objective Search and Constraint Solving for Configuring Large Software Product Lines}, 
year={2015}, 
volume={1}, 
pages={517-528}, 
keywords={configuration management;optimisation;search problems;software metrics;software product lines;SATIBEA framework;SPL feature selection;constraint solving;diversity metrics;multiobjective search-based optimization;software product lines configuration;Filtering algorithms;Frequency modulation;Measurement;Optimization;Search problems;Software;Software product lines}, 
doi={10.1109/ICSE.2015.69}, 
ISSN={0270-5257}, 
month={May},}

@inproceedings{metzger14,
  title={Software product line engineering and variability management: achievements and challenges},
  author={Metzger, Andreas and Pohl, Klaus},
  booktitle={Proc. on Future of Software Engineering},
  pages={70--84},
  year={2014},
  organization={ACM}
}

@book{clements2002software,
  title={Software product lines},
  author={Clements, Paul and Northrop, Linda},
  year={2002},
  publisher={Addison-Wesley,}
}

@article{nair17,
  title={Using Bad Learners to find Good Configurations},
  author={Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
  journal={arXiv preprint arXiv:1702.05701},
  year={2017}
}

@inproceedings{me09m,
annote = {Available from  http://menzies.us/pdf/09fssga.pdf},
author = {Andrews, Jamie and Menzies, Tim},
booktitle = {PROMISE'09},
title = {{On the Value of Combining Feature Subset Selection with Genetic Algorithms: Faster Learning of Coverage Models}},
year = {2009}
}

@inproceedings{andrews07,
annote = {Available from  http://menzies.us/pdf/07ase-nighthawk.pdf},
author = {Andrews, J H and Li, F C H and Menzies, T},
booktitle = {IEEE ASE'07},
title = {{Nighthawk: A Two-Level Genetic-Random Unit Test Data Generator}},
year = {2007}
}

@article{andrews10,
author = {Andrews, James H and Menzies, Tim and Li, Felix C H},
journal = {IEEE Transactions on Software Engineering},
month = mar,
title = {{Genetic Algorithms for Randomized Unit Testing}},
year = {2010}
}

@inproceedings{me07f,
  title={The business case for automated software engineering},
  author={Menzies, Tim and Elrawas, Oussama and Hihn, Jairus and Feather, Martin and Madachy, Ray and Boehm, Barry},
  booktitle={Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering},
  pages={303--312},
  year={2007},
  organization={ACM}
}


@inproceedings{zimm09,
  title={Cross-project defect prediction: a large scale experiment on data vs. domain vs. process},
  author={Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
  booktitle={Proc. the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
  pages={91--100},
  year={2009},
  organization={ACM}
}

@book{boehm00b,
author = {Boehm, Barry and Horowitz, Ellis and Madachy, Ray and Reifer, Donald and Clark, Bradford K and Steece, Bert and Brown, A Winsor and Chulani, Sunita and Abts, Chris},
publisher = {Prentice Hall},
title = {{Software Cost Estimation with Cocomo II}},
year = {2000}
}

@book{boehm81,
author = {Boehm, B},
publisher = {Prentice Hall},
title = {{Software Engineering Economics}},
year = {1981}
}

@inproceedings{me09i,
  title={Understanding the value of software engineering technologies},
  author={Ii, Phillip Green and Menzies, Tim and Williams, Steven and El-Rawas, Oussama},
  booktitle={Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering},
  pages={52--61},
  year={2009},
  organization={IEEE Computer Society}
}

@inproceedings{me09j,
author = {Lemon, B and Riesbeck, A and Menzies, T and Price, J and D'Alessandro, J and Carlsson, R and Prifiti, T and Peters, F and Lu, H and Port, D},
booktitle = {IEEE ASE'09},
title = {{Applications of Simulation and AI Search: Assessing the Relative Merits of Agile vs Traditional Software Development}},
year = {2009}
}

@inproceedings{fea02a,
author = {Feather, M S and Menzies, T},
booktitle = {IEEE Joint Conf. Requirements Engineering ICRE'02 and RE'02, 9-13th September, University of Essen, Germany},
title = {{Converging on the Optimal Attainment of Requirements}},
year = {2002}
}

@article{Cui2005a,
author = {Cui, X and Potok, Te and Palathingal, P},
file = {:Users/timm/svns/doc/pso/05clusterPSO.pdf:pdf},
journal = {\ldots  Intelligence Symposium, 2005. \ldots},
title = {{Document clustering using particle swarm optimization}},
year = {2005}
}

@article{LeGoues2015,
author = {{Le Goues}, Claire and Holtschulte, Neal and Smith, Edward K and Brun, Yuriy and Devanbu, Premkumar and Forrest, Stephanie and Weimer, Westley},
doi = {10.1109/TSE.2015.2454513},
file = {:home/rkrsn/Documents/Mendeley Desktop/Le Goues et al. - 2015 - The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Automated program repair,INTROCLASS,MANYBUGS,benchmark,reproducibility,subject defect},
mendeley-groups = {Automated Program Repair},
month = {dec},
number = {12},
pages = {1236--1256},
title = {{The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs}},
volume = {41},
year = {2015}
}

@inproceedings{Schulte2010,
abstract = {A method is described for automatically repairing legacy software at the assembly code level using evolutionary computation. The technique is demonstrated on Java byte code and x86 assembly programs, showing how to find program variations that correct defects while retaining desired behavior. Test cases are used to demonstrate the defect and define required functionality. The paper explores advantages of assembly-level repair over earlier work at the source code level—the ability to repair programs written in many different languages; and the ability to repair bugs that were previously intractable. The paper reports experimental results showing reasonable performance of assembly language repair even on non-trivial programs},
address = {New York, New York, USA},
author = {Schulte, Eric and Forrest, Stephanie and Weimer, Westley},
booktitle = {Proc. IEEE/ACM Intl. Conf. Automated software engineering - ASE '10},
doi = {10.1145/1858996.1859059},
file = {:home/rkrsn/Documents/Mendeley Desktop/Schulte, Forrest, Weimer - 2010 - Automated Program Repair through the Evolution of Assembly Code.pdf:pdf},
isbn = {9781450301169},
keywords = {assembly code,bytecode,evolutionary computation,fault localiza-,legacy software,program repair,tion},
mendeley-groups = {Automated Program Repair},
pages = {313},
publisher = {ACM Press},
title = {{Automated program repair through the evolution of assembly code}},
year = {2010}
}

@inproceedings{Weimer2009,
abstract = {Automatic program repair has been a longstanding goal in software engineering, yet debugging remains a largely manual process. We introduce a fully automated method for locating and repairing bugs in software. The approach works on off-the-shelf legacy applications and does not require formal specifications, program annotations or special coding practices. Once a program fault is discovered, an extended form of genetic programming is used to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Standard test cases are used to exercise the fault and to encode program requirements. After a successful repair has been discovered, it is minimized using structural differencing algorithms and delta debugging. We describe the proposed method and report experimental results demonstrating that it can successfully repair ten different C programs totaling 63,000 lines in under 200 seconds, on average.},
author = {Weimer, Westley and Nguyen, ThanhVu and {Le Goues}, Claire and Forrest, Stephanie},
booktitle = {Proc. Intl. Conf. Software Engineering},
doi = {10.1109/ICSE.2009.5070536},
isbn = {9781424434527},
issn = {02705257},
keywords = {[Electronic Manuscript]},
mendeley-groups = {Automated Program Repair},
pages = {364--374},
publisher = {IEEE},
title = {{Automatically finding patches using genetic programming}},
year = {2009}
}

@inproceedings{Forrest2009,
abstract = {Genetic programming is combined with program analysis methods to repair bugs in off-the-shelf legacy C programs. Fitness is defined using negative test cases that exercise the bug to be repaired and positive test cases that encode program requirements. Once a successful repair is discovered, structural differencing algorithms and delta debugging methods are used to minimize its size. Several modifications to the GP technique contribute to its success: (1) genetic operations are localized to the nodes along the execution path of the negative test case; (2) high-level statements are represented as single nodes in the program tree; (3) genetic operators use existing code in other parts of the program, so new code does not need to be invented. The paper describes the method, reviews earlier experiments that repaired 11 bugs in over 60,000 lines of code, reports results on new bug repairs, and describes experiments that analyze the performance and efficacy of the evolutionary components of the algorithm.},
address = {New York, New York, USA},
author = {Forrest, Stephanie and Nguyen, ThanhVu and Weimer, Westley and {Le Goues}, Claire},
booktitle = {Proc. 11th Annual Conf. Genetic and evolutionary computation - GECCO '09},
doi = {10.1145/1569901.1570031},
file = {:home/rkrsn/Documents/Mendeley Desktop/Forrest, Weimer, Goues - Unknown - A Genetic Programming Approach to Automated Software Repair Categories and Subject Descriptors.pdf:pdf},
isbn = {9781605583259},
issn = {1089778X},
keywords = {genetic programming,software engineering,software repair},
mendeley-groups = {Automated Program Repair},
pages = {947},
publisher = {ACM Press},
title = {{A genetic programming approach to automated software repair}},
year = {2009}
}

@inproceedings{Goues12,
abstract = {There are more bugs in real-world programs than human programmers can realistically address. This paper evaluates two research questions: What fraction of bugs can be repaired automatically? and How much does it cost to repair a bug automatically? In previous work, we presented GenProg, which uses genetic programming to repair defects in off-the-shelf C programs. To answer these questions, we: (1) propose novel algorithmic improvements to GenProg that allow it to scale to large programs and find repairs 68{\%} more often, (2) exploit GenProgs inherent parallelism using cloud computing resources to provide grounded, human- competitive cost measurements, and (3) generate a large, indicative benchmark set to use for systematic evaluations. We evaluate GenProg on 105 defects from 8 open-source programs totaling 5.1 million lines of code and involving 10,193 test cases. GenProg automatically repairs 55 of those 105 defects. To our knowledge, this evaluation is the largest available of its kind, and is often two orders of magnitude larger than previous work in terms of code or test suite size or defect count. Public cloud computing prices allow our 105 runs to be reproduced for {\$}403; a successful repair completes in 96 minutes and costs {\$}7.32, on average.},
author = {{Le Goues}, Claire and Dewey-Vogt, Michael and Forrest, Stephanie and Weimer, Westley},
booktitle = {2012 34th Intl. Conf. Software Engineering (ICSE)},
doi = {10.1109/ICSE.2012.6227211},
file = {:home/rkrsn/Documents/Mendeley Desktop/Le Goues et al. - 2012 - A systematic study of automated program repair Fixing 55 out of 105 bugs for {\$}8 each.pdf:pdf},
isbn = {978-1-4673-1066-6},
issn = {02705257},
keywords = {automated program repair,cloud computing,genetic programming},
mendeley-groups = {Automated Program Repair},
month = {jun},
pages = {3--13},
publisher = {IEEE},
title = {{A systematic study of automated program repair: Fixing 55 out of 105 bugs for {\$}8 each}},
year = {2012}
}


@article{deb14,
author = {Deb, K and Jain, H},
doi = {10.1109/TEVC.2013.2281535},
file = {:Users/timm/svns/doc/13nsga-III.pdf:pdf},
issn = {1089-778X},
journal = {Evolutionary Computation, IEEE Transactions on},
keywords = {genetic algorithms;sorting;EMO algorithms;MOEA/D m},
month = aug,
number = {4},
pages = {577--601},
title = {{An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints}},
volume = {18},
year = {2014}
}

@article{zhang07:TEC,
author = {Zhang, Qingfu and Li, Hui},
doi = {10.1109/TEVC.2007.892759},
issn = {1089-778X},
journal = {Evolutionary Computation, IEEE Transactions on},
keywords = {computational complexity;genetic algorithms;comput},
month = dec,
number = {6},
pages = {712--731},
title = {{MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition}},
volume = {11},
year = {2007}
}

@article{krall2015gale,
  title={Gale: Geometric active learning for search-based software engineering},
  author={Krall, Joseph and Menzies, Tim and Davies, Misty},
  journal={IEEE Transactions on Software Engineering},
  volume={41},
  number={10},
  pages={1001--1018},
  year={2015},
  publisher={IEEE}
}

@incollection{zit04,
author = {Zitzler, Eckart and K\"{u}nzli, Simon},
booktitle = {Parallel Problem Solving from Nature - PPSN VIII},
doi = {10.1007/978-3-540-30217-9\_84},
isbn = {978-3-540-23092-2},
pages = {832--842},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Indicator-Based Selection in Multiobjective Search}},
volume = {3242},
year = {2004}
}

@inproceedings{zit02,
author = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar},
booktitle = {Evolutionary Methods for Design, Optimisation, and Control},
pages = {95--100},
publisher = {CIMNE, Barcelona, Spain},
title = {{SPEA2: Improving the Strength Pareto Evolutionary Algorithm for Multiobjective Optimization}},
year = {2002}
}


@article{menzies07,
  title={Data mining static code attributes to learn defect predictors},
  author={Menzies, Tim and Greenwald, Jeremy and Frank, Art},
  journal={IEEE transactions on software engineering},
  volume={33},
  number={1},
  pages={2--13},
  year={2007},
  publisher={IEEE}
}

@ARTICLE{jorgensen09,
  author =	 {J{\o}rgensen, Magne and Gruschke, Tanja M.},
  journal =	 {Software Engineering, IEEE Transactions on},
  title =	 {The Impact of Lessons-Learned Sessions on Effort
                  Estimation and Uncertainty Assessments},
  year =	 2009,
  month =	 {May-June },
  volume =	 35,
  number =	 3,
  pages =	 {368 -383},
}

@inproceedings{passos11,
  title =	 "Analyzing the Impact of Beliefs in Software Project
                  Practices",
  author =	 "Carol Passos and Ana Paula Braun and Daniela
                  S. Cruzes and Manoel Mendonca",
  year =	 2011,
  booktitle =	 "ESEM'11"
}

@inproceedings {prem16,
  title={Belief \& evidence in empirical software engineering},
  author={Devanbu, Prem and Zimmermann, Thomas and Bird, Christian},
  booktitle={Proc. 38th Intl. Conf. Software Engineering},
  pages={108--119},
  year={2016},
  organization={ACM}
}

@article{cortes95,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer Netherlands}
}
@article{dtrees,
  title={Induction of decision trees},
  author={Quinlan, J Ross},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}

@TECHREPORT{spea2,
    author = {Eckart Zitzler and Marco Laumanns and Lothar Thiele},
    title = {SPEA2: Improving the Strength Pareto Evolutionary Algorithm},
    institution = {TIK-Report 103},
    year = {2001}
}

@article{deb2002fast,
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II},
  author={Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, TAMT},
  journal={IEEE transactions on evolutionary computation},
  volume={6},
  number={2},
  pages={182--197},
  year={2002},
  publisher={IEEE}
}
@Inbook{Nair2016,
author="Nair, Vivek
and Menzies, Tim
and Chen, Jianfeng",
title="An (Accidental) Exploration of Alternatives to Evolutionary Algorithms for SBSE",
bookTitle="Search Based Software Engineering: 8th Intl. Symposium, SSBSE 2016, Raleigh, NC, USA, October 8-10, 2016, Proceedings",
year="2016",
publisher="Springer Intl. Publishing",
address="Cham",
pages="96--111",
abstract="SBSE researchers often use an evolutionary algorithm to solve various software engineering problems. This paper explores an alternate approach of sampling. This approach is called SWAY (Samplying WAY) and finds the (near) optimal solutions to the problem by (i) creating a larger initial population and (ii) intelligently sampling the solution space to find the best subspace. Unlike evolutionary algorithms, SWAY does not use mutation or cross-over or multi-generational reasoning to find interesting subspaces but relies on the underlying dimensions of the solution space. Experiments with Software Engineering (SE) models shows that SWAY's performance improvement is competitive with standard MOEAs while, terminating over an order of magnitude faster.",
isbn="978-3-319-47106-8",
doi="10.1007/978-3-319-47106-8_7"
}
@article{gale,
  author    = {Joseph Krall and
               Tim Menzies and
               Misty Davies},
  title     = {{GALE:} Geometric Active Learning for Search-Based Software Engineering},
  journal   = {{IEEE} Trans. Software Eng.},
  volume    = {41},
  number    = {10},
  pages     = {1001--1018},
  year      = {2015},
  doi       = {10.1109/TSE.2015.2432024},
  timestamp = {Thu, 15 Jun 2017 21:30:51 +0200},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{rahman12,
 author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar},
 title = {Recalling the "Imprecision" of Cross-project Defect Prediction},
 booktitle = {Proc. ACM SIGSOFT 20th Intl. Symposium on the Foundations of Software Engineering},
 series = {FSE '12},
 year = {2012},
 isbn = {978-1-4503-1614-9},
 location = {Cary, North Carolina},
 pages = {61:1--61:11},
 articleno = {61},
 numpages = {11},
 doi = {10.1145/2393596.2393669},
 acmid = {2393669},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {empirical software engineering, fault prediction, inspection},
}
@article{tallam2006concept,
  title={A concept analysis inspired greedy algorithm for test suite minimization},
  author={Tallam, Sriraman and Gupta, Neelam},
  journal={ACM SIGSOFT Software Engineering Notes},
  volume={31},
  number={1},
  pages={35--42},
  year={2006},
  publisher={ACM}
}
@article{yoo2012regression,
  title={Regression testing minimization, selection and prioritization: a survey},
  author={Yoo, Shin and Harman, Mark},
  journal={Software Testing, Verification and Reliability},
  volume={22},
  number={2},
  pages={67--120},
  year={2012},
  publisher={Wiley Online Library}
}
@inproceedings{blue2013interaction,
  title={Interaction-based test-suite minimization},
  author={Blue, Dale and Segall, Itai and Tzoref-Brill, Rachel and Zlotnick, Aviad},
  booktitle={Proc. the 2013 Intl. Conf. Software Engineering},
  pages={182--191},
  year={2013},
  organization={IEEE Press}
}

@inproceedings{nayrolles2018clever,
  title={CLEVER: Combining Code Metrics with Clone Detection for Just-In-Time Fault Prevention and Resolution in Large Industrial Projects},
  author={Nayrolles, Mathieu and Hamou-Lhadj, Abdelwahab},
  booktitle={Mining Software Repositories},
  year={2018}
}

@inproceedings{krishna2017b,
  title={Learning effective changes for software projects},
  author={Krishna, Rahul},
  booktitle={Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
  pages={1002--1005},
  year={2017},
  organization={IEEE Press}
}

@inproceedings{ruhe2003quantitative,
  title={Quantitative studies in software release planning under risk and resource constraints},
  author={Ruhe, G{\"u}nther and Greer, Des},
  booktitle={Empirical Software Engineering, 2003. ISESE 2003. Proceedings. 2003 Intl. Symposium on},
  pages={262--270},
  year={2003},
  organization={IEEE}
}
@book{ruhe2010product,
  title={Product release planning: methods, tools and applications},
  author={Ruhe, G{\"u}nther},
  year={2010},
  publisher={CRC Press}
}

@article{elish2011,
  title={A classification of refactoring methods based on software quality attributes},
  author={Elish, Karim and Alshayeb, Mohammad},
  journal={Arabian Journal for Science and Engineering},
  volume={36},
  number={7},
  pages={1253--1267},
  year={2011},
  publisher={Springer}
}
@article{elish2012,
  title={Using Software Quality Attributes to Classify Refactoring to Patterns.},
  author={Elish, Karim and Alshayeb, Mohammad},
  journal={JSW},
  volume={7},
  number={2},
  pages={408--419},
  year={2012}
}

@inproceedings{stroggylos2007,
  title={Refactoring--Does It Improve Software Quality?},
  author={Stroggylos, Konstantinos and Spinellis, Diomidis},
  booktitle={Software Quality, 2007. WoSQ'07: ICSE Workshops 2007. Fifth Intl. Workshop on},
  pages={10--10},
  year={2007},
  organization={IEEE}
}

@misc{du2006study,
  title={A study of quality improvements by refactoring},
  author={Du Bois, Bart},
  year={2006}
}

@inproceedings{kataoka2002,
  title={A quantitative evaluation of maintainability enhancement by refactoring},
  author={Kataoka, Yoshio and Imai, Takeo and Andou, Hiroki and Fukaya, Tetsuji},
  booktitle={Software Maintenance, 2002. Proceedings. Intl. Conf.},
  pages={576--585},
  year={2002},
  organization={IEEE}
}

@inproceedings{bryton2009,
  title={Strengthening refactoring: towards software evolution with quantitative and experimental grounds},
  author={Bryton, S{\'e}rgio and e Abreu, Fernando Brito},
  booktitle={Software Engineering Advances, 2009. ICSEA'09. Fourth Intl. Conf.},
  pages={570--575},
  year={2009},
  organization={IEEE}
}

@article{krishna2017less,
  title={Less is more: Minimizing code reorganization using XTREE},
  author={Krishna, Rahul and Menzies, Tim and Layman, Lucas},
  journal={Information and Software Technology},
  volume={88},
  pages={53--66},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{devanbu2016belief,
  title={Belief \& evidence in empirical software engineering},
  author={Devanbu, Premkumar and Zimmermann, Thomas and Bird, Christian},
  booktitle={Software Engineering (ICSE), 2016 IEEE/ACM 38th Intl. Conf.},
  pages={108--119},
  year={2016},
  organization={IEEE}
}

@inproceedings{Nam2013,
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. © 2013 IEEE.},
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
booktitle = {Proc. Intl. Conf. Software Engineering},
doi = {10.1109/ICSE.2013.6606584},
isbn = {9781467330763},
issn = {02705257},
keywords = {cross-project defect prediction,empirical software engineering,transfer learning},
pages = {382--391},
title = {{Transfer defect learning}},
year = {2013}
}

@inproceedings{Nam2015,
address = {New York, New York, USA},
author = {Nam, Jaechang and Kim, Sunghun},
booktitle = {Proc. 2015 10th Jt. Meet. Found. Softw. Eng. - ESEC/FSE 2015},
doi = {10.1145/2786805.2786814},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Nam, Kim - Heterogeneous defect prediction - 2015.pdf:pdf},
isbn = {9781450336758},
keywords = {defect prediction,heterogeneous metrics,quality assurance},
mendeley-groups = {ASE 2016},
pages = {508--519},
publisher = {ACM Press},
title = {{Heterogeneous defect prediction}},
year = {2015}
}

@inproceedings{jing15,
title="Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning",
author=" X. Jing and G. Wu and X. Dong and F. Qi and B. Xu",
year =2015,
booktitle="FSE'15"
}

@inproceedings{kocaguneli2011find,
title={How to find relevant data for effort estimation?},
author={Kocaguneli, Ekrem and Menzies, Tim},
booktitle={Empirical Software Engineering and Measurement (ESEM), 2011 Intl. Symposium on},
pages={255--264},
year={2011},
organization={IEEE}
}

@article{kocaguneli2012,
author = {Kocaguneli, Ekrem and Menzies, Tim and Mendes, Emilia},
doi = {10.1007/s10664-014-9300-5},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {jun},
number = {3},
pages = {813--843},
publisher = {Springer US},
title = {{Transfer learning in effort estimation}},
volume = {20},
year = {2015}
}

@inproceedings{peters15,
abstract = {Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data. In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute "interesting" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).},
author = {Peters, Fayola and Menzies, Tim and Layman, Lucas},
booktitle = {Proc. Intl. Conf. Software Engineering},
doi = {10.1109/ICSE.2015.92},
isbn = {9781479919345},
issn = {02705257},
pages = {801--811},
title = {{LACE2: Better privacy-preserving data sharing for cross project defect prediction}},
volume = {1},
year = {2015}
}


@article{mensah2018investigating,
  title={Investigating the Significance of the Bellwether Effect to Improve Software Effort Prediction: Further Empirical Study},
  author={Mensah, Solomon and Keung, Jacky and MacDonell, Stephen G and Bosu, Michael Franklin and Bennin, Kwabena Ebo},
  journal={IEEE Transactions on Reliability},
  number={99},
  pages={1--23},
  year={2018},
  publisher={IEEE}
}

@article{levandowsky71,
  title={Distance between sets},
  author={Levandowsky, Michael and Winter, David},
  journal={Nature},
  volume={234},
  number={5323},
  pages={34},
  year={1971},
  publisher={Nature Publishing Group}
}